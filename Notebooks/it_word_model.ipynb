{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"it_word_model.ipynb","provenance":[{"file_id":"101cRuXF63VnSailkZxz6HOPRwf7N6Aqe","timestamp":1607023883174},{"file_id":"1CrMNVEbQcgGqqwsxAJN6WrFZmnliJ6zj","timestamp":1606876939705}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cm3RTgaDeHf4","executionInfo":{"status":"ok","timestamp":1607374108525,"user_tz":300,"elapsed":8257,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"ae933ccb-f9fe-4826-e02a-c94214fc30b2"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Z5m7NTyeV5g","executionInfo":{"status":"ok","timestamp":1607374108525,"user_tz":300,"elapsed":6672,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"665dbc51-69d5-4bfa-c8ce-27e273cce3b0"},"source":["cd /content/drive/MyDrive/NLP_Proj/"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1N_RbAQL8cE4O8RGgJnoCl82t6V9afaB-/NLP_Proj\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PB4Tv4_Iec9b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607374116677,"user_tz":300,"elapsed":6181,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"61191c5e-8420-4454-d122-bdafcd89d11c"},"source":["!pip install jsonlines"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting jsonlines\n","  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from jsonlines) (1.15.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-1.2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6hNGvn2regB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607374117931,"user_tz":300,"elapsed":7060,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"8f355ea5-b18d-498d-c525-7232b5175e62"},"source":["!pip install tqdm"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NE3qfjYVdzO6","executionInfo":{"status":"ok","timestamp":1607374120094,"user_tz":300,"elapsed":9032,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}}},"source":["import os\n","import jsonlines\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import LSTM\n","from torch.nn import Embedding\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import numpy as np\n","from numpy import inf\n","import matplotlib.pyplot as plt\n","import pickle\n","import time"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5blZllFRrNh","executionInfo":{"status":"ok","timestamp":1607374120095,"user_tz":300,"elapsed":8815,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}}},"source":["class Dictionary(object): #maps words to indices\n","  def __init__(self, datasets, include_valid=False):\n","      self.tokens = []\n","      self.ids = {}\n","      self.counts = {}\n","\n","      # add special tokens\n","      self.add_token('<bos>') #beginning of sentence\n","      self.add_token('<eos>') #end of sentence\n","      self.add_token('<pad>')\n","      self.add_token('<unk>') #unknown. Needed in case use with text with word that isn't in vocab\n","\n","      for line in tqdm(datasets['train']):\n","          for w in line:\n","              self.add_token(w)\n","\n","      if include_valid is True:\n","          for line in tqdm(datasets['valid']):\n","              for w in line:\n","                  self.add_token(w)\n","\n","  def add_token(self, w):\n","      if w not in self.tokens:\n","          self.tokens.append(w)\n","          _w_id = len(self.tokens) - 1\n","          self.ids[w] = _w_id\n","          self.counts[w] = 1\n","      else:\n","          self.counts[w] += 1\n","\n","  def get_id(self, w):\n","      return self.ids[w]\n","\n","  def get_token(self, idx):\n","      return self.tokens[idx]\n","\n","  def decode_idx_seq(self, l):\n","      return [self.tokens[i] for i in l]\n","\n","  def encode_token_seq(self, l):\n","      return [self.ids[i] if i in self.ids else self.ids['<unk>'] for i in l]\n","\n","  def __len__(self):\n","      return len(self.tokens)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyARbVKkeDzr","executionInfo":{"status":"ok","timestamp":1607374120096,"user_tz":300,"elapsed":8708,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}}},"source":["def load_pickle(path):\n","    with open(path, 'rb') as handle:\n","        tokenized_datasets = pickle.load(handle)\n","    return tokenized_datasets\n","\n","def pad_strings(minibatch):\n","    max_len_sample = max(len(i.split(' ')) for i in minibatch)\n","    result = []\n","    for line in minibatch:\n","        line_len = len(line.split(' '))\n","        padding_str = ' ' + '<pad> ' * (max_len_sample - line_len)\n","        result.append(line + padding_str)\n","    return result\n","\n","class TensoredDataset():\n","    def __init__(self, list_of_lists_of_tokens):\n","        self.input_tensors = []\n","        self.target_tensors = []\n","\n","        for sample in list_of_lists_of_tokens:\n","            self.input_tensors.append(torch.tensor([sample[:-1]], dtype=torch.long))\n","            self.target_tensors.append(torch.tensor([sample[1:]], dtype=torch.long))\n","\n","    def __len__(self):\n","        return len(self.input_tensors)\n","\n","    def __getitem__(self, idx):\n","        # return a (input, target) tuple\n","        # f=0\n","        # if f==0:\n","        #   print(idx)\n","        #   print('len input',len(self.input_tensors))\n","        #   print('len target',len(self.target_tensors))\n","        #   f=1\n","        return (self.input_tensors[idx], self.target_tensors[idx])\n","\n","def pad_list_of_tensors(list_of_tensors, pad_token):\n","    max_length = max([t.size(-1) for t in list_of_tensors])\n","    padded_list = []\n","    for t in list_of_tensors:\n","        padded_tensor = torch.cat([t, torch.tensor([[pad_token] * (max_length - t.size(-1))], dtype=torch.long)],\n","                                  dim=-1)\n","        padded_list.append(padded_tensor)\n","\n","    padded_tensor = torch.cat(padded_list, dim=0)\n","    return padded_tensor\n","\n","def pad_collate_fn(batch):\n","    input_list = [s[0] for s in batch]\n","    target_list = [s[1] for s in batch]\n","    pad_token = 2 # wiki_dict.get_id('<pad>')\n","    input_tensor = pad_list_of_tensors(input_list, pad_token)\n","    target_tensor = pad_list_of_tensors(target_list, pad_token)\n","    # f=0\n","    # if f==0:\n","    #   print(input_tensor.size())\n","    #   f=1\n","    return input_tensor, target_tensor\n","\n","class LSTMLanguageModel(nn.Module):\n","    \"\"\"\n","    This model combines embedding, lstm and projection layer into a single model\n","    \"\"\"\n","    def __init__(self, options):\n","        super().__init__()\n","\n","        self.lookup = nn.Embedding(num_embeddings=options['num_embeddings'], embedding_dim=options['embedding_dim'], padding_idx=options['padding_idx'])\n","        self.lstm = nn.LSTM(options['input_size'], options['hidden_size'], options['num_layers'], dropout=options['lstm_dropout'], batch_first=True)\n","        self.projection = nn.Linear(options['hidden_size'], options['num_embeddings'])\n","\n","    def forward(self, encoded_input_sequence):\n","        \"\"\"\n","        Forward method process the input from token ids to logits\n","        \"\"\"\n","        # print('encoded_input_sequence')\n","        # print(encoded_input_sequence.size())\n","        # print(encoded_input_sequence[0])\n","        embeddings = self.lookup(encoded_input_sequence)\n","        # print('embeddings')\n","        # print(embeddings.size())\n","        # print(embeddings[0])\n","        lstm_outputs = self.lstm(embeddings)\n","        logits = self.projection(lstm_outputs[0])\n","\n","        return logits\n","\n","def model_training(model, optimizer, num_epochs):\n","  plot_cache = []\n","  best_loss = float(inf)\n","  no_improvement = 0\n","\n","  \n","  for epoch_number in range(num_epochs):\n","      torch.cuda.empty_cache()\n","      avg_loss=0\n","      model.train()\n","      train_log_cache = []\n","      start_time = time.time()\n","      for i, (inp, target) in enumerate(wiki_loaders['train']):\n","          optimizer.zero_grad()\n","          inp = inp.to(current_device)\n","          target = target.to(current_device)\n","          logits = model(inp)\n","          loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n","          loss.backward()\n","          optimizer.step()\n","          train_log_cache.append(loss.item())\n","          torch.cuda.empty_cache()\n","      if current_device == 'cuda':\n","          print(torch.cuda.get_device_name(0))\n","          print('Memory Usage:')\n","          print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","          print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n","      avg_loss = sum(train_log_cache)/len(train_log_cache)\n","      torch.cuda.empty_cache()\n","      print('Training loss after {} epoch = {:.{prec}f}'.format(epoch_number+1, avg_loss, prec=4))\n","      print(time.time()-start_time)\n","\n","      valid_losses = []\n","      model.eval()\n","      with torch.no_grad():\n","        for i, (inp, target) in enumerate(wiki_loaders['valid']):\n","            inp = inp.to(current_device)\n","            target = target.to(current_device)\n","            logits = model(inp)\n","\n","            loss = criterion(logits.view(-1, logits.size(-1)), target.view(-1))\n","            valid_losses.append(loss.item())\n","        avg_val_loss = sum(valid_losses) / len(valid_losses)\n","        torch.cuda.empty_cache()\n","        print('Validation loss after {} epoch = {:.{prec}f}'.format(epoch_number+1, avg_val_loss, prec=4))\n","\n","        if (avg_val_loss < best_loss):\n","          best_loss = avg_val_loss\n","        else:\n","          no_improvement += 1\n","\n","        if(no_improvement >= 5):\n","          print('Early stopping at epoch: %d', epoch_number+1)\n","          break\n","      plot_cache.append((avg_loss, avg_val_loss))\n","\n","  return plot_cache, best_loss\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tLKuEoBnra_","executionInfo":{"status":"ok","timestamp":1607374120096,"user_tz":300,"elapsed":8465,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}}},"source":["# LANG (str): ar, en, it, hi\n","# TYPE (str): CHAR or WORD\n","# NUM_EPOCHS (int): number of epochs to train for\n","# BATCH_SIZE (int): batch size\n","\n","LANG = 'it'\n","USE_CHARS = False\n","NUM_EPOCHS = 30 #10\n","BATCH_SIZE = 64\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UtUxnKclR3tL","executionInfo":{"status":"ok","timestamp":1607374130163,"user_tz":300,"elapsed":18308,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"d27a0507-05d2-45a4-acb8-364d17ee0eed"},"source":["type = 'char' if USE_CHARS == True else 'word'\n","print('model language and type:', LANG, type)\n","\n","batch_size = BATCH_SIZE\n","print('batch size:', batch_size)\n","\n","num_gpus = torch.cuda.device_count()\n","if num_gpus > 0:\n","    current_device = 'cuda'\n","else:\n","    current_device = 'cpu'\n","print('device:', current_device)\n","if current_device == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n","\n","############################################################################\n","######################### AFTER GENERATING PICKLE ##########################\n","############################################################################\n","PATH = LANG+'_'+type+'_tokenized.pickle'\n","\n","wiki_loaders = {}\n","\n","print('start loading')\n","wiki_tokenized_datasets = load_pickle(path=PATH)\n","print('done loading')\n","wiki_path = LANG+'_'+type+'_wiki_dict_filtered.pickle'\n","# wiki_path = LANG+'_'+type+'_wiki_dict.pickle'\n","with open(wiki_path, 'rb') as handle:\n","    wiki_dict = pickle.load(handle)\n","print(len(wiki_dict.ids))\n","\n","wiki_tensor_dataset = {}\n","\n","for split, listoflists in wiki_tokenized_datasets.items():\n","    wiki_tensor_dataset[split] = TensoredDataset(listoflists)\n","\n","for split, wiki_dataset in wiki_tensor_dataset.items():\n","    wiki_loaders[split] = DataLoader(wiki_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n","\n","embedding_size = int(256)\n","hidden_size = int(1024/2)\n","num_layers = 3\n","lstm_dropout = 0.3\n","if USE_CHARS:\n","    num_embeddings = len(wiki_dict.ids)\n","if (not USE_CHARS):\n","    num_embeddings = len(wiki_dict.ids)\n","\n","options = {\n","    'num_embeddings': num_embeddings, # number of characters/words + eos, bos, unk, pad\n","    'embedding_dim': embedding_size,\n","    'padding_idx': 2, #wiki_dict.get_id('<pad>')\n","    'input_size': embedding_size,\n","    'hidden_size': hidden_size,\n","    'num_layers': num_layers,\n","    'lstm_dropout': lstm_dropout,\n","}\n","print(options)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["model language and type: it word\n","batch size: 64\n","device: cuda\n","Tesla P100-PCIE-16GB\n","Memory Usage:\n","Allocated: 0.0 GB\n","Cached:    0.0 GB\n","start loading\n","done loading\n","70508\n","{'num_embeddings': 70508, 'embedding_dim': 256, 'padding_idx': 2, 'input_size': 256, 'hidden_size': 512, 'num_layers': 3, 'lstm_dropout': 0.3}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJdjysGV9hp0","executionInfo":{"status":"ok","timestamp":1607374310043,"user_tz":300,"elapsed":845,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"22efc617-aa9c-404f-dee4-7b08ab166e24"},"source":["# Check training data sample\n","wiki_tensor_dataset['train'][0][0].size(),wiki_tensor_dataset['train'][0][1].size(),"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 17]), torch.Size([1, 17]))"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"-q98F0aUjd2o"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JXjw2XI8eupp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607198861419,"user_tz":300,"elapsed":1687697,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"14debd3c-ff08-42ba-967e-b376e29d8182"},"source":["LSTM_model = LSTMLanguageModel(options).to(current_device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=2) #wiki_dict.get_id('<pad>')\n","\n","model_parameters = [p for p in LSTM_model.parameters() if p.requires_grad]\n","optimizer = optim.SGD(model_parameters, lr=0.001, momentum=0.999)\n","filename = './saved_models/LSTM_'+LANG+'_'+type+'_'+str(BATCH_SIZE)+'bsize_'+str(NUM_EPOCHS)+'ep.pt'\n","# filename = './saved_models/LSTM_'+LANG+'_'+type+'_'+str(BATCH_SIZE)+'bsize_'+str(embedding_size)+'emb_'+str(hidden_size)+'hdim_'+str(num_layers)+'lyrs_'+str(NUM_EPOCHS)+'ep.pt'\n","plot, loss = model_training(model=LSTM_model, optimizer=optimizer, num_epochs=NUM_EPOCHS)\n","torch.save({'model_state_dict': LSTM_model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'plot_cache': plot,\n","            'loss': loss,\n","            }, filename)\n","print(filename)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 1 epoch = 7.7298\n","52.250049114227295\n","Validation loss after 1 epoch = 6.7575\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 2 epoch = 6.5283\n","52.13889837265015\n","Validation loss after 2 epoch = 6.3487\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.7 GB\n","Training loss after 3 epoch = 6.2697\n","52.46939516067505\n","Validation loss after 3 epoch = 6.1619\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 4 epoch = 6.0941\n","52.53784251213074\n","Validation loss after 4 epoch = 6.0076\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 5 epoch = 5.9587\n","52.486321687698364\n","Validation loss after 5 epoch = 5.8963\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 6 epoch = 5.8526\n","52.501412868499756\n","Validation loss after 6 epoch = 5.8062\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 7 epoch = 5.7650\n","52.236518144607544\n","Validation loss after 7 epoch = 5.7302\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 8 epoch = 5.6886\n","52.57872939109802\n","Validation loss after 8 epoch = 5.6656\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 9 epoch = 5.6196\n","52.22080612182617\n","Validation loss after 9 epoch = 5.6074\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 10 epoch = 5.5534\n","52.46674871444702\n","Validation loss after 10 epoch = 5.5442\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 11 epoch = 5.4894\n","52.70811867713928\n","Validation loss after 11 epoch = 5.4888\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.8 GB\n","Training loss after 12 epoch = 5.4255\n","55.47620177268982\n","Validation loss after 12 epoch = 5.4318\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 13 epoch = 5.3640\n","55.808764696121216\n","Validation loss after 13 epoch = 5.3834\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 14 epoch = 5.3036\n","55.620097398757935\n","Validation loss after 14 epoch = 5.3302\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 15 epoch = 5.2455\n","55.63348984718323\n","Validation loss after 15 epoch = 5.2827\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.7 GB\n","Training loss after 16 epoch = 5.1895\n","55.55320930480957\n","Validation loss after 16 epoch = 5.2421\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.9 GB\n","Training loss after 17 epoch = 5.1379\n","55.35804867744446\n","Validation loss after 17 epoch = 5.2071\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.5 GB\n","Cached:    1.0 GB\n","Training loss after 18 epoch = 5.0902\n","55.484290599823\n","Validation loss after 18 epoch = 5.1698\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 19 epoch = 5.0463\n","55.6507453918457\n","Validation loss after 19 epoch = 5.1293\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.8 GB\n","Training loss after 20 epoch = 5.0026\n","55.44062328338623\n","Validation loss after 20 epoch = 5.1068\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.9 GB\n","Training loss after 21 epoch = 4.9624\n","55.64410161972046\n","Validation loss after 21 epoch = 5.0783\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 22 epoch = 4.9248\n","55.64524531364441\n","Validation loss after 22 epoch = 5.0567\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 23 epoch = 4.8891\n","54.55717849731445\n","Validation loss after 23 epoch = 5.0328\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 24 epoch = 4.8549\n","53.183085441589355\n","Validation loss after 24 epoch = 5.0119\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 25 epoch = 4.8224\n","53.17421054840088\n","Validation loss after 25 epoch = 4.9891\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 26 epoch = 4.7911\n","55.41101145744324\n","Validation loss after 26 epoch = 4.9749\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.7 GB\n","Training loss after 27 epoch = 4.7629\n","55.60748100280762\n","Validation loss after 27 epoch = 4.9608\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 28 epoch = 4.7346\n","55.425649881362915\n","Validation loss after 28 epoch = 4.9469\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.3 GB\n","Cached:    0.7 GB\n","Training loss after 29 epoch = 4.7093\n","55.91220998764038\n","Validation loss after 29 epoch = 4.9334\n","Tesla V100-SXM2-16GB\n","Memory Usage:\n","Allocated: 0.4 GB\n","Cached:    0.7 GB\n","Training loss after 30 epoch = 4.6833\n","55.70327281951904\n","Validation loss after 30 epoch = 4.9232\n","./saved_models/LSTM_it_word_64bsize_30ep.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tuJbFYlxiAwJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607198861712,"user_tz":300,"elapsed":1684811,"user":{"displayName":"Francesca Guiso","photoUrl":"","userId":"10973502609023098424"}},"outputId":"942dca52-a011-44e1-d19c-d679c313e9bd"},"source":["filename = './saved_models/LSTM_'+LANG+'_'+type+'_'+str(BATCH_SIZE)+'bsize_'+str(embedding_size)+'emb_'+str(hidden_size)+'hdim_'+str(num_layers)+'lyrs_'+str(NUM_EPOCHS)+'ep.pt'\n","torch.save({'model_state_dict': LSTM_model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'plot_cache': plot,\n","            'loss': loss,\n","            }, filename)\n","print(filename)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["./saved_models/LSTM_it_word_64bsize_256emb_512hdim_3lyrs_30ep.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fq0QBWSxPmWS"},"source":[""],"execution_count":null,"outputs":[]}]}